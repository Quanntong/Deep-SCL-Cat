import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, roc_auc_score
import warnings
import os
import sys

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥configæ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import src.config as config
except ImportError:
    # å¦‚æœç›´æ¥å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç›¸å¯¹å¯¼å…¥
    try:
        from . import config
    except ImportError:
        # æœ€åå°è¯•ç›´æ¥å¯¼å…¥config
        import config

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')


def run_comparison(X_train, y_train, X_test, y_test, our_model):
    """
    è¿è¡Œå¤šæ¨¡å‹å¯¹æ¯”åˆ†æ
    
    å‚æ•°:
    X_train: è®­ç»ƒé›†ç‰¹å¾
    y_train: è®­ç»ƒé›†æ ‡ç­¾
    X_test: æµ‹è¯•é›†ç‰¹å¾
    y_test: æµ‹è¯•é›†æ ‡ç­¾
    our_model: å·²è®­ç»ƒå¥½çš„CatBoostæ¨¡å‹
    
    è¿”å›:
    DataFrame: åŒ…å«æ‰€æœ‰æ¨¡å‹æ€§èƒ½æŒ‡æ ‡çš„å¯¹æ¯”ç»“æœ
    """
    print("=" * 60)
    print("å¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ")
    print("=" * 60)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    outputs_dir = os.path.join(config.BASE_DIR, 'outputs')
    os.makedirs(outputs_dir, exist_ok=True)
    
    # å®šä¹‰åŸºçº¿æ¨¡å‹
    baselines = {}
    
    # 1. éšæœºæ£®æ—
    try:
        baselines["Random Forest"] = RandomForestClassifier(
            n_estimators=100, 
            random_state=config.RANDOM_SEED,
            n_jobs=-1
        )
        print("âœ“ å·²æ·»åŠ  Random Forest æ¨¡å‹")
    except Exception as e:
        print(f"âœ— Random Forest æ¨¡å‹æ·»åŠ å¤±è´¥: {e}")
    
    # 2. XGBoost
    try:
        from xgboost import XGBClassifier
        baselines["XGBoost"] = XGBClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6,
            random_state=config.RANDOM_SEED,
            use_label_encoder=False,
            eval_metric='logloss',
            n_jobs=-1
        )
        print("âœ“ å·²æ·»åŠ  XGBoost æ¨¡å‹")
    except ImportError:
        print("âœ— XGBoost æœªå®‰è£…ï¼Œè·³è¿‡æ­¤æ¨¡å‹")
    except Exception as e:
        print(f"âœ— XGBoost æ¨¡å‹æ·»åŠ å¤±è´¥: {e}")
    
    # 3. LightGBM
    try:
        from lightgbm import LGBMClassifier
        baselines["LightGBM"] = LGBMClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6,
            random_state=config.RANDOM_SEED,
            n_jobs=-1
        )
        print("âœ“ å·²æ·»åŠ  LightGBM æ¨¡å‹")
    except ImportError:
        print("âœ— LightGBM æœªå®‰è£…ï¼Œè·³è¿‡æ­¤æ¨¡å‹")
    except Exception as e:
        print(f"âœ— LightGBM æ¨¡å‹æ·»åŠ å¤±è´¥: {e}")
    
    # 4. é€»è¾‘å›å½’ï¼ˆéœ€è¦æ ‡å‡†åŒ–ï¼‰
    try:
        baselines["Logistic Regression"] = make_pipeline(
            StandardScaler(),
            LogisticRegression(
                max_iter=1000,
                random_state=config.RANDOM_SEED,
                n_jobs=-1
            )
        )
        print("âœ“ å·²æ·»åŠ  Logistic Regression æ¨¡å‹")
    except Exception as e:
        print(f"âœ— Logistic Regression æ¨¡å‹æ·»åŠ å¤±è´¥: {e}")
    
    # 5. SVMï¼ˆéœ€è¦æ ‡å‡†åŒ–ï¼‰
    try:
        baselines["SVM"] = make_pipeline(
            StandardScaler(),
            SVC(
                probability=True,
                random_state=config.RANDOM_SEED
            )
        )
        print("âœ“ å·²æ·»åŠ  SVM æ¨¡å‹")
    except Exception as e:
        print(f"âœ— SVM æ¨¡å‹æ·»åŠ å¤±è´¥: {e}")
    
    print(f"\næ€»å…±æ·»åŠ äº† {len(baselines)} ä¸ªåŸºçº¿æ¨¡å‹")
    
    # å­˜å‚¨ç»“æœ
    results = []
    
    # è®­ç»ƒå’Œè¯„ä¼°åŸºçº¿æ¨¡å‹
    print("\n" + "-" * 60)
    print("å¼€å§‹è®­ç»ƒå’Œè¯„ä¼°åŸºçº¿æ¨¡å‹...")
    print("-" * 60)
    
    for model_name, model in baselines.items():
        try:
            print(f"\nè®­ç»ƒ {model_name}...")
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_train, y_train)
            
            # é¢„æµ‹
            y_pred = model.predict(X_test)
            y_pred_proba = None
            
            # å°è¯•è·å–é¢„æµ‹æ¦‚ç‡ï¼ˆç”¨äºAUCè®¡ç®—ï¼‰
            try:
                if hasattr(model, 'predict_proba'):
                    y_pred_proba = model.predict_proba(X_test)[:, 1]
                elif hasattr(model, 'decision_function'):
                    # å¯¹äºSVMç­‰æ¨¡å‹ï¼Œä½¿ç”¨decision_function
                    y_pred_proba = model.decision_function(X_test)
            except:
                y_pred_proba = None
            
            # è®¡ç®—æŒ‡æ ‡
            recall = recall_score(y_test, y_pred, average='weighted')
            precision = precision_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')
            accuracy = accuracy_score(y_test, y_pred)
            
            # è®¡ç®—AUCï¼ˆå¦‚æœæœ‰æ¦‚ç‡é¢„æµ‹ï¼‰
            auc = np.nan
            if y_pred_proba is not None:
                try:
                    auc = roc_auc_score(y_test, y_pred_proba)
                except:
                    auc = np.nan
            
            # å­˜å‚¨ç»“æœ
            results.append({
                'Model': model_name,
                'Recall': recall,
                'Precision': precision,
                'F1': f1,
                'Accuracy': accuracy,
                'AUC': auc
            })
            
            print(f"  Recall: {recall:.4f}, F1: {f1:.4f}, Accuracy: {accuracy:.4f}, AUC: {auc:.4f}")
            
        except Exception as e:
            print(f"  âœ— {model_name} è®­ç»ƒæˆ–è¯„ä¼°å¤±è´¥: {e}")
            # æ·»åŠ å¤±è´¥è®°å½•
            results.append({
                'Model': model_name,
                'Recall': np.nan,
                'Precision': np.nan,
                'F1': np.nan,
                'Accuracy': np.nan,
                'AUC': np.nan
            })
    
    # è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼ˆDeep-SCL-Catï¼‰
    print("\n" + "-" * 60)
    print("è¯„ä¼° Deep-SCL-Cat æ¨¡å‹...")
    print("-" * 60)
    
    try:
        # ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        y_pred_our = our_model.predict(X_test)
        y_pred_proba_our = our_model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        recall_our = recall_score(y_test, y_pred_our, average='weighted')
        precision_our = precision_score(y_test, y_pred_our, average='weighted')
        f1_our = f1_score(y_test, y_pred_our, average='weighted')
        accuracy_our = accuracy_score(y_test, y_pred_our)
        auc_our = roc_auc_score(y_test, y_pred_proba_our)
        
        # å­˜å‚¨æˆ‘ä»¬çš„æ¨¡å‹ç»“æœ
        results.append({
            'Model': 'Deep-SCL-Cat (Ours)',
            'Recall': recall_our,
            'Precision': precision_our,
            'F1': f1_our,
            'Accuracy': accuracy_our,
            'AUC': auc_our
        })
        
        print(f"  Recall: {recall_our:.4f}, F1: {f1_our:.4f}, Accuracy: {accuracy_our:.4f}, AUC: {auc_our:.4f}")
        
    except Exception as e:
        print(f"  âœ— Deep-SCL-Cat æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
        # æ·»åŠ å¤±è´¥è®°å½•
        results.append({
            'Model': 'Deep-SCL-Cat (Ours)',
            'Recall': np.nan,
            'Precision': np.nan,
            'F1': np.nan,
            'Accuracy': np.nan,
            'AUC': np.nan
        })
    
    # åˆ›å»ºç»“æœDataFrame
    results_df = pd.DataFrame(results)
    
    # æŒ‰Recallé™åºæ’åº
    results_df_sorted = results_df.sort_values('Recall', ascending=False).reset_index(drop=True)
    
    # ä¿å­˜ç»“æœåˆ°CSV
    csv_path = os.path.join(outputs_dir, 'model_comparison.csv')
    results_df_sorted.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
    
    # å¯è§†åŒ–
    print("\n" + "-" * 60)
    print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("-" * 60)
    
    try:
        # å‡†å¤‡æ•°æ®ç”¨äºå¯è§†åŒ–
        plot_data = results_df_sorted.copy()
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8-darkgrid')
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('å¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
        
        # æŒ‡æ ‡åˆ—è¡¨
        metrics = ['Recall', 'F1', 'Accuracy', 'AUC']
        metric_titles = ['å¬å›ç‡ (Recall)', 'F1åˆ†æ•°', 'å‡†ç¡®ç‡ (Accuracy)', 'AUC']
        
        # ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ›å»ºæ¡å½¢å›¾
        for idx, (metric, title) in enumerate(zip(metrics, metric_titles)):
            ax = axes[idx // 2, idx % 2]
            
            # æ’åºæ•°æ®
            plot_metric = plot_data.dropna(subset=[metric]).sort_values(metric, ascending=True)
            
            # åˆ›å»ºæ¡å½¢å›¾
            bars = ax.barh(plot_metric['Model'], plot_metric[metric], 
                          color=plt.cm.Set3(np.linspace(0, 1, len(plot_metric))))
            
            # é«˜äº®æˆ‘ä»¬çš„æ¨¡å‹
            for i, (model_name, bar) in enumerate(zip(plot_metric['Model'], bars)):
                if 'Deep-SCL-Cat' in model_name:
                    bar.set_color('red')
                    bar.set_edgecolor('darkred')
                    bar.set_linewidth(2)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (value, bar) in enumerate(zip(plot_metric[metric], bars)):
                ax.text(value + 0.01, bar.get_y() + bar.get_height()/2, 
                       f'{value:.3f}', 
                       va='center', ha='left', fontsize=9)
            
            ax.set_xlabel(title, fontsize=12)
            ax.set_xlim(0, min(1.0, plot_metric[metric].max() * 1.2))
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ ç½‘æ ¼çº¿
            ax.xaxis.grid(True, linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        img_path = os.path.join(outputs_dir, 'model_comparison.png')
        plt.savefig(img_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {img_path}")
        
        # æ˜¾ç¤ºå›¾è¡¨
        plt.show()
        
    except Exception as e:
        print(f"âœ— å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("æ¨¡å‹å¯¹æ¯”æ€»ç»“")
    print("=" * 60)
    print(results_df_sorted.to_string(index=False))
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    if not results_df_sorted.empty:
        best_model = results_df_sorted.iloc[0]
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹ (åŸºäºRecall): {best_model['Model']}")
        print(f"   å¬å›ç‡: {best_model['Recall']:.4f}, F1åˆ†æ•°: {best_model['F1']:.4f}")
    
    return results_df_sorted


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=" * 60)
    print("æ¨¡å‹å¯¹æ¯”æ¨¡å—æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
    np.random.seed(config.RANDOM_SEED)
    n_samples = 1000
    n_features = 20
    
    X_train = np.random.randn(n_samples, n_features)
    y_train = np.random.randint(0, 2, n_samples)
    X_test = np.random.randn(200, n_features)
    y_test = np.random.randint(0, 2, 200)
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„CatBoostæ¨¡å‹
    from catboost import CatBoostClassifier
    dummy_model = CatBoostClassifier(
        iterations=10,
        random_seed=config.RANDOM_SEED,
        verbose=False
    )
    dummy_model.fit(X_train, y_train)
    
    # è¿è¡Œå¯¹æ¯”
    results = run_comparison(X_train, y_train, X_test, y_test, dummy_model)
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ!")
    print("=" * 60)
